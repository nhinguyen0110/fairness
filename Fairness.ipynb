{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1+UzA1o0hxdMULGVI3n1y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"p9EXDV_VwipR"},"outputs":[],"source":["#@title Import revelant modules and install Facets\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from matplotlib import pyplot as plt\n","from matplotlib import rcParams\n","import seaborn as sns\n","\n","# The following lines adjust the granularity of reporting.\n","pd.options.display.max_rows = 10\n","pd.options.display.float_format = \"{:.1f}\".format\n","\n","from google.colab import widgets\n","# For facets\n","from IPython.core.display import display, HTML\n","import base64\n","!pip install facets-overview==1.0.0\n","from facets_overview.feature_statistics_generator import FeatureStatisticsGenerator"]},{"cell_type":"code","source":["COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n","           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n","           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n","           \"income_bracket\"]\n","\n","train_csv = tf.keras.utils.get_file('adult.data',\n","  'https://download.mlcc.google.com/mledu-datasets/adult_census_train.csv')\n","test_csv = tf.keras.utils.get_file('adult.test' ,\n","  'https://download.mlcc.google.com/mledu-datasets/adult_census_test.csv')\n","\n","train_df = pd.read_csv(train_csv, names=COLUMNS, sep=r'\\s*,\\s*',\n","                       engine='python', na_values=\"?\")\n","test_df = pd.read_csv(test_csv, names=COLUMNS, sep=r'\\s*,\\s*', skiprows=[0],\n","                      engine='python', na_values=\"?\")\n","# Strip trailing periods mistakenly included only in UCI test dataset.\n","test_df['income_bracket'] = test_df.income_bracket.str.rstrip('.')"],"metadata":{"id":"wWZOuNBGxZOR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Visualize the Data in Facets\n","fsg = FeatureStatisticsGenerator()\n","dataframes = [\n","    {'table': train_df, 'name': 'trainData'}]\n","censusProto = fsg.ProtoFromDataFrames(dataframes)\n","protostr = base64.b64encode(censusProto.SerializeToString()).decode(\"utf-8\")\n","\n","\n","HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n","        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n","        <facets-overview id=\"elem\"></facets-overview>\n","        <script>\n","          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n","        </script>\"\"\"\n","html = HTML_TEMPLATE.format(protostr=protostr)\n","display(HTML(html))"],"metadata":{"id":"fvekPGRexcOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Set the Number of Data Points to Visualize in Facets Dive\n","\n","SAMPLE_SIZE = 5000 #@param\n","\n","train_dive = train_df.sample(SAMPLE_SIZE).to_json(orient='records')\n","\n","HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n","        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\">\n","        <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n","        <script>\n","          var data = {jsonstr};\n","          document.querySelector(\"#elem\").data = data;\n","        </script>\"\"\"\n","html = HTML_TEMPLATE.format(jsonstr=train_dive)\n","display(HTML(html))"],"metadata":{"id":"FnjnQRHrxgcN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature = 'gender' #@param [\"\", \"hours_per_week\", \"fnlwgt\", \"gender\", \"capital_gain / capital_loss\", \"age\"] {allow-input: false}\n","\n","\n","if feature == \"hours_per_week\":\n","  print(\n","'''It does seem a little strange to see 'hours_per_week' max out at 99 hours,\n","which could lead to data misrepresentation. One way to address this is by\n","representing 'hours_per_week' as a binary \"working 40 hours/not working 40\n","hours\" feature. Also keep in mind that data was extracted based on work hours\n","being greater than 0. In other words, this feature representation exclude a\n","subpopulation of the US that is not working. This could skew the outcomes of the\n","model.''')\n","if feature == \"fnlwgt\":\n","  print(\n","\"\"\"'fnlwgt' represents the weight of the observations. After fitting the model\n","to this data set, if certain group of individuals end up performing poorly\n","compared to other groups, then we could explore ways of reweighting each data\n","point using this feature.\"\"\")\n","if feature == \"gender\":\n","  print(\n","\"\"\"Looking at the ratio between men and women shows how disproportionate the data\n","is compared to the real world where the ratio (at least in the US) is closer to\n","1:1. This could pose a huge probem in performance across gender. Considerable\n","measures may need to be taken to upsample the underrepresented group (in this\n","case, women).\"\"\")\n","if feature == \"capital_gain / capital_loss\":\n","  print(\n","\"\"\"As alluded to in Task #1, both 'capital_gain' and 'capital_loss' could be\n","indicative of income status as only individuals who make investments register\n","their capital gains and losses. The caveat is that over 90% of the values in\n","both 'capital_gain' and 'capital_loss' are 0, and it's not entirely clear from\n","the description of the data set why that is the case. That is, we don't know\n","whether we should interpret all these 0s as \"no investment gain/loss or \"\n","investment gain/loss is unknown.\" Lack of context is always a flag for concern,\n","and one that could trigger fairness-related issues later on. For now, we are\n","going to omit these features from the model, but you are more than welcome to\n","experiment with them if you come up with an idea on how capital gains and\n","losses should be handled.\"\"\")\n","if feature == \"age\":\n","  print(\n","'''\"age\" has a lot of variance, so it might benefit from bucketing to learn\n","fine-grained correlations between income and age, as well as to prevent\n","overfitting.''')"],"metadata":{"id":"b55YH0o3xlT-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pandas_to_numpy(data):\n","  '''Convert a pandas DataFrame into a Numpy array'''\n","  # Drop empty rows.\n","  data = data.dropna(how=\"any\", axis=0)\n","\n","  # Separate DataFrame into two Numpy arrays.\n","  labels = np.array(data['income_bracket'] == \">50K\")\n","  features = data.drop('income_bracket', axis=1)\n","  features = {name:np.array(value) for name, value in features.items()}\n","\n","  return features, labels"],"metadata":{"id":"Q_5-OAaIxoPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Create categorical feature columns\n","\n","# Since we don't know the full range of possible values with occupation and\n","# native_country, we'll use categorical_column_with_hash_bucket() to help map\n","# each feature string into an integer ID.\n","occupation = tf.feature_column.categorical_column_with_hash_bucket(\n","    \"occupation\", hash_bucket_size=1000)\n","native_country = tf.feature_column.categorical_column_with_hash_bucket(\n","    \"native_country\", hash_bucket_size=1000)\n","\n","# For the remaining categorical features, since we know what the possible values\n","# are, we can be more explicit and use categorical_column_with_vocabulary_list()\n","gender = tf.feature_column.categorical_column_with_vocabulary_list(\n","    \"gender\", [\"Female\", \"Male\"])\n","race = tf.feature_column.categorical_column_with_vocabulary_list(\n","    \"race\", [\n","        \"White\", \"Asian-Pac-Islander\", \"Amer-Indian-Eskimo\", \"Other\", \"Black\"\n","    ])\n","education = tf.feature_column.categorical_column_with_vocabulary_list(\n","    \"education\", [\n","        \"Bachelors\", \"HS-grad\", \"11th\", \"Masters\", \"9th\",\n","        \"Some-college\", \"Assoc-acdm\", \"Assoc-voc\", \"7th-8th\",\n","        \"Doctorate\", \"Prof-school\", \"5th-6th\", \"10th\", \"1st-4th\",\n","        \"Preschool\", \"12th\"\n","    ])\n","marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n","    \"marital_status\", [\n","        \"Married-civ-spouse\", \"Divorced\", \"Married-spouse-absent\",\n","        \"Never-married\", \"Separated\", \"Married-AF-spouse\", \"Widowed\"\n","    ])\n","relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n","    \"relationship\", [\n","        \"Husband\", \"Not-in-family\", \"Wife\", \"Own-child\", \"Unmarried\",\n","        \"Other-relative\"\n","    ])\n","workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n","    \"workclass\", [\n","        \"Self-emp-not-inc\", \"Private\", \"State-gov\", \"Federal-gov\",\n","        \"Local-gov\", \"?\", \"Self-emp-inc\", \"Without-pay\", \"Never-worked\"\n","    ])"],"metadata":{"id":"gZ7DbZAcxqc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Create numeric feature columns\n","# For Numeric features, we can just call on feature_column.numeric_column()\n","# to use its raw value instead of having to create a map between value and ID.\n","age = tf.feature_column.numeric_column(\"age\")\n","fnlwgt = tf.feature_column.numeric_column(\"fnlwgt\")\n","education_num = tf.feature_column.numeric_column(\"education_num\")\n","capital_gain = tf.feature_column.numeric_column(\"capital_gain\")\n","capital_loss = tf.feature_column.numeric_column(\"capital_loss\")\n","hours_per_week = tf.feature_column.numeric_column(\"hours_per_week\")"],"metadata":{"id":"bwO2NhtvxtOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["age_buckets = tf.feature_column.bucketized_column(\n","    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])"],"metadata":{"id":"J27lwHPXxxS2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List of variables, with special handling for gender subgroup.\n","variables = [native_country, education, occupation, workclass,\n","             relationship, age_buckets]\n","subgroup_variables = [gender]\n","feature_columns = variables + subgroup_variables"],"metadata":{"id":"dlihB11oxzIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["deep_columns = [\n","    tf.feature_column.indicator_column(workclass),\n","    tf.feature_column.indicator_column(education),\n","    tf.feature_column.indicator_column(age_buckets),\n","    tf.feature_column.indicator_column(relationship),\n","    tf.feature_column.embedding_column(native_country, dimension=8),\n","    tf.feature_column.embedding_column(occupation, dimension=8),\n","]"],"metadata":{"id":"j017Dk_ox1TG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define Deep Neural Net Model\n","\n","# Parameters from form fill-ins\n","HIDDEN_UNITS_LAYER_01 = 128 #@param\n","HIDDEN_UNITS_LAYER_02 = 64 #@param\n","LEARNING_RATE = 0.1 #@param\n","L1_REGULARIZATION_STRENGTH = 0.001 #@param\n","L2_REGULARIZATION_STRENGTH = 0.001 #@param\n","\n","RANDOM_SEED = 512\n","tf.random.set_seed(RANDOM_SEED)\n","\n","# List of built-in metrics that we'll need to evaluate performance.\n","METRICS = [\n","  tf.keras.metrics.TruePositives(name='tp'),\n","  tf.keras.metrics.FalsePositives(name='fp'),\n","  tf.keras.metrics.TrueNegatives(name='tn'),\n","  tf.keras.metrics.FalseNegatives(name='fn'),\n","  tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n","  tf.keras.metrics.Precision(name='precision'),\n","  tf.keras.metrics.Recall(name='recall'),\n","  tf.keras.metrics.AUC(name='auc'),\n","]\n","\n","regularizer = tf.keras.regularizers.l1_l2(\n","    l1=L1_REGULARIZATION_STRENGTH, l2=L2_REGULARIZATION_STRENGTH)\n","\n","model = tf.keras.Sequential([\n","  layers.DenseFeatures(deep_columns),\n","  layers.Dense(\n","      HIDDEN_UNITS_LAYER_01, activation='relu', kernel_regularizer=regularizer),\n","  layers.Dense(\n","      HIDDEN_UNITS_LAYER_02, activation='relu', kernel_regularizer=regularizer),\n","  layers.Dense(\n","      1, activation='sigmoid', kernel_regularizer=regularizer)\n","])\n","\n","model.compile(optimizer=tf.keras.optimizers.Adagrad(LEARNING_RATE),\n","              loss=tf.keras.losses.BinaryCrossentropy(),\n","              metrics=METRICS)"],"metadata":{"id":"Fuc7ejY1x3vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Fit Deep Neural Net Model to the Adult Training Dataset\n","\n","EPOCHS = 10 #@param\n","BATCH_SIZE = 500 #@param\n","\n","features, labels = pandas_to_numpy(train_df)\n","model.fit(x=features, y=labels, epochs=EPOCHS, batch_size=BATCH_SIZE)"],"metadata":{"id":"DoS7lRJPx7po"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Evaluate Deep Neural Net Performance\n","\n","features, labels = pandas_to_numpy(test_df)\n","model.evaluate(x=features, y=labels);"],"metadata":{"id":"-5_3uzIYx-C_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Define Function to Visualize Binary Confusion Matrix\n","def plot_confusion_matrix(\n","    confusion_matrix, class_names, subgroup, figsize = (8,6)):\n","  # We're taking our calculated binary confusion matrix that's already in the\n","  # form of an array and turning it into a pandas DataFrame because it's a lot\n","  # easier to work with a pandas DataFrame when visualizing a heat map in\n","  # Seaborn.\n","  df_cm = pd.DataFrame(\n","      confusion_matrix, index=class_names, columns=class_names,\n","  )\n","\n","  rcParams.update({\n","  'font.family':'sans-serif',\n","  'font.sans-serif':['Liberation Sans'],\n","  })\n","\n","  sns.set_context(\"notebook\", font_scale=1.25)\n","\n","  fig = plt.figure(figsize=figsize)\n","\n","  plt.title('Confusion Matrix for Performance Across ' + subgroup)\n","\n","  # Combine the instance (numercial value) with its description\n","  strings = np.asarray([['True Positives', 'False Negatives'],\n","                        ['False Positives', 'True Negatives']])\n","  labels = (np.asarray(\n","      [\"{0:g}\\n{1}\".format(value, string) for string, value in zip(\n","          strings.flatten(), confusion_matrix.flatten())])).reshape(2, 2)\n","\n","  heatmap = sns.heatmap(df_cm, annot=labels, fmt=\"\",\n","      linewidths=2.0, cmap=sns.color_palette(\"GnBu_d\"));\n","  heatmap.yaxis.set_ticklabels(\n","      heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n","  heatmap.xaxis.set_ticklabels(\n","      heatmap.xaxis.get_ticklabels(), rotation=45, ha='right')\n","  plt.ylabel('References')\n","  plt.xlabel('Predictions')\n","  return fig"],"metadata":{"id":"km28SSA6yD6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Visualize Binary Confusion Matrix and Compute Evaluation Metrics Per Subgroup\n","CATEGORY  =  \"gender\" #@param {type:\"string\"}\n","SUBGROUP =  \"Male\" #@param {type:\"string\"}\n","\n","# Labels for annotating axes in plot.\n","classes = ['Over $50K', 'Less than $50K']\n","\n","# Given define subgroup, generate predictions and obtain its corresponding\n","# ground truth.\n","subgroup_filter  = test_df.loc[test_df[CATEGORY] == SUBGROUP]\n","features, labels = pandas_to_numpy(subgroup_filter)\n","subgroup_results = model.evaluate(x=features, y=labels, verbose=0)\n","confusion_matrix = np.array([[subgroup_results[1], subgroup_results[4]],\n","                             [subgroup_results[2], subgroup_results[3]]])\n","\n","subgroup_performance_metrics = {\n","    'ACCURACY': subgroup_results[5],\n","    'PRECISION': subgroup_results[6],\n","    'RECALL': subgroup_results[7],\n","    'AUC': subgroup_results[8]\n","}\n","performance_df = pd.DataFrame(subgroup_performance_metrics, index=[SUBGROUP])\n","pd.options.display.float_format = '{:,.4f}'.format\n","\n","plot_confusion_matrix(confusion_matrix, classes, SUBGROUP);\n","performance_df"],"metadata":{"id":"IRLzCDRgyE7o"},"execution_count":null,"outputs":[]}]}